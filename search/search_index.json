{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"K-NEAREST NEIGHBOR \u00b6 Pengertian K-NEAREST NEIGHBOR \u00b6 Algoritme k-nearest neighbor (k-NN atau KNN) adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data pembelajaran. Sebuah titik pada ruang ini ditandai kelas c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean. Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah k buah yang paling dekat diambil. Titik yang baru klasifikasinya diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik tersebut. Nilai k yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritme nearest neighbor . Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu). Algoritma K-NEAREST NEIGHBOR \u00b6 terdapat beberapa data yang menunjukkan wilayah kependudukan berdasarkan latitude dan longitude (atau garis lintang dan garis bujur) sebagai posisi untuk menentukan wilayah tersebut termasuk kota atau kabupaten pada wilayah bandung Dari data diatas, kita mendapatkan beberapa informasi, diantaranya: Independent Variables , yaitu variable yang nilainya tidak dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk independent variable adalah Lat , dan Long . Dependent Variables , yaitu variable yang nilainya dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk dependent variable adalah Lokasi . Rumah A-E adalah rumah yang masuk ke dalam wilayah Kota . Rumah F-G adalah rumah yang masuk ke dalam wilayah Kabupaten . Rumah X adalah rumah yang akan kita prediksi menggunakan algoritma kNN apakah termasuk ke dalam wilayah Kota atau Kabupaten. Didalam dunia Machine Learning , Independent Variables sering disebut juga sebagai Features . Selanjutnya kita hitung jarak antara rumah X terhadap rumah A-G dengan menggunakan rumus pythagoras : Diketahui, dimana x adalah Lat , y adalah Long , sedangkan (x1, y1) adalah lat dan long dari rumah X , dan (x2, y2) adalah lat dan long dari masing-masing tetangganya . Setelah dihitung, selanjutnya adalah urutkan jarak tersebut dari yang paling kecil ke yang paling besar , hasilnya adalah sebagai berikut: Dapat dilihat dari hasil perhitungan diatas, bahwa ternyata 3 tetangga terdekat dari rumah X adalah: Rumah H (Kabupaten) yang memiliki jarak 2.24 , Rumah C (Kota) yang memiliki jarak 3 , dan Rumah E (Kota) yang memiliki jarak 3.16 . Dari ke-3 tetangga terdekat, terdapat 2 rumah yang termasuk kedalam wilayah Kota dan 1 rumah yang masuk ke dalam wilayah Kabupaten . Sehingga dapat disimpulkan, bahwa Rumah X adalah rumah yang termasuk kedalam wilayah Kota Bandung . Implementasi K-NEAREST NEIGHBOR pada python \u00b6 hal yang diperlukan dalam pengimplementasian python 3.6 download data pada laman berikut https://github.com/BULLZHIT/data/tree/master/knn Algoritma K-Mean Clustering pada python \u00b6 setelah syarat terpunuhi kita dapat memulai pada langkah selanjutnya Import Data Hal pertama yang perlu kita lakukan adalah memuat file data kita. Data dalam format CSV tanpa baris header atau kutipan. Kita dapat membuka file dengan fungsi terbuka dan membaca baris data menggunakan fungsi pembaca dalam modul csv. import csv with open('iris.data', 'rb') as csvfile: lines = csv.reader(csvfile) for row in lines: print ', '.join(row) Selanjutnya kita perlu membagi data menjadi dataset training yang dapat digunakan kNN untuk membuat prediksi dan dataset data testing yang dapat kita gunakan untuk mengevaluasi akurasi model. Pertama-tama kita perlu mengubah ukuran bunga yang dimuat sebagai string menjadi angka yang dapat kita kerjakan. Selanjutnya kita perlu membagi set data secara acak menjadi kereta dan dataset. Rasio 67/33 untuk ujicoba/ tes adalah rasio standar yang digunakan. Dengan menggabungkan semuanya, kita dapat membuat fungsi yang disebut loadDataset yang memuat CSV dengan nama file yang disediakan dan membaginya secara acak ke dalam train dan menguji dataset menggunakan rasio split yang disediakan. import csv import random def loadDataset(filename, split, trainingSet=[] , testSet=[]): with open(filename, 'rb') as csvfile: lines = csv.reader(csvfile) dataset = list(lines) for x in range(len(dataset)-1): for y in range(4): dataset[x][y] = float(dataset[x][y]) if random.random() < split: trainingSet.append(dataset[x]) else: testSet.append(dataset[x]) Unduh file CSV dataset dataset bunga iris ke direktori lokal. Kami dapat menguji fungsi ini dengan set data iris kami, sebagai berikut: trainingSet=[] testSet=[] loadDataset('iris.data', 0.66, trainingSet, testSet) print 'Train: ' + repr(len(trainingSet)) print 'Test: ' + repr(len(testSet)) Kesamaan Untuk membuat prediksi kita perlu menghitung kesamaan antara dua instance data yang diberikan. Ini diperlukan agar kita dapat menemukan contoh instans k yang paling mirip dalam dataset pelatihan untuk anggota tertentu dari dataset pengujian dan pada gilirannya membuat prediksi. Mengingat keempat pengukuran bunga bersifat numerik dan memiliki satuan yang sama, kita dapat langsung menggunakan ukuran jarak Euclidean. Ini didefinisikan sebagai akar kuadrat dari jumlah perbedaan kuadrat antara dua array angka (baca lagi beberapa kali dan biarkan meresap). Selain itu, kami ingin mengontrol bidang mana yang akan dimasukkan dalam perhitungan jarak. Secara khusus, kami hanya ingin menyertakan 4 atribut pertama. Salah satu pendekatan adalah membatasi jarak euclidean ke panjang tetap, mengabaikan dimensi akhir. Menyatukan semua ini, kita dapat mendefinisikan fungsi euclideanDistance sebagai berikut: import math def euclideanDistance(instance1, instance2, length): distance = 0 for x in range(length): distance += pow((instance1[x] - instance2[x]), 2) return math.sqrt(distance) kita dapat menguji fungsi ini dengan beberapa data sampel, sebagai berikut: data1 = [2, 2, 2, 'a'] data2 = [4, 4, 4, 'b'] distance = euclideanDistance(data1, data2, 3) print 'Distance: ' + repr(distance) Tetangga Sekarang kita memiliki ukuran kesamaan, kita dapat menggunakannya mengumpulkan k contoh yang paling mirip untuk contoh yang tidak terlihat. Ini adalah proses langsung menghitung jarak untuk semua instance dan memilih subset dengan nilai jarak terkecil. Di bawah ini adalah fungsi getNeighbors yang mengembalikan k paling mirip tetangga dari set pelatihan untuk contoh uji yang diberikan (menggunakan fungsi euclideanDistance yang sudah ditentukan) import operator def getNeighbors(trainingSet, testInstance, k): distances = [] length = len(testInstance)-1 for x in range(len(trainingSet)): dist = euclideanDistance(testInstance, trainingSet[x], length) distances.append((trainingSet[x], dist)) distances.sort(key=operator.itemgetter(1)) neighbors = [] for x in range(k): neighbors.append(distances[x][0]) return neighbors Kita dapat menguji fungsi ini sebagai berikut: trainSet = [[2, 2, 2, 'a'], [4, 4, 4, 'b']] testInstance = [5, 5, 5] k = 1 neighbors = getNeighbors(trainSet, testInstance, 1) print(neighbors) Respon Setelah kami menemukan tetangga yang paling mirip untuk contoh pengujian, tugas selanjutnya adalah menyusun respons yang diprediksi berdasarkan tetangga tersebut. Kita dapat melakukan ini dengan mengizinkan setiap tetangga untuk memilih atribut kelas mereka, dan mengambil suara mayoritas sebagai prediksi. Di bawah ini menyediakan fungsi untuk mendapatkan respons suara terbanyak dari sejumlah tetangga. Diasumsikan kelas adalah atribut terakhir untuk setiap tetangga. import operator def getResponse(neighbors): classVotes = {} for x in range(len(neighbors)): response = neighbors[x][-1] if response in classVotes: classVotes[response] += 1 else: classVotes[response] = 1 sortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedVotes[0][0] Kami dapat menguji fungsi ini dengan beberapa tetangga pengujian, sebagai berikut: neighbors = [[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']] response = getResponse(neighbors) print(response) Akurasi Kami memiliki semua bagian dari algoritma kNN di tempatnya. Kekhawatiran penting yang tersisa adalah bagaimana mengevaluasi keakuratan prediksi. Cara mudah untuk mengevaluasi keakuratan model adalah dengan menghitung rasio total prediksi yang benar dari semua prediksi yang dibuat, yang disebut akurasi klasifikasi. Di bawah ini adalah fungsi getAccuracy yang menjumlahkan total prediksi yang benar dan mengembalikan akurasi sebagai persentase dari klasifikasi yang benar. def getAccuracy(testSet, predictions): correct = 0 for x in range(len(testSet)): if testSet[x][-1] is predictions[x]: correct += 1 return (correct/float(len(testSet))) * 100.0 kita dapat menguji fungsi ini dengan dataset uji dan prediksi, sebagai berikut: testSet = [[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']] predictions = ['a', 'a', 'a'] accuracy = getAccuracy(testSet, predictions) print(accuracy) Main Kita sekarang memiliki semua elemen algoritma dan Kita dapat mengikatnya bersama dengan fungsi utama. Di bawah ini adalah contoh lengkap penerapan algoritma kNN dari awal dengan Python. # Example of kNN implemented from Scratch in Python import csv import random import math import operator def loadDataset(filename, split, trainingSet=[] , testSet=[]): with open(filename, 'rb') as csvfile: lines = csv.reader(csvfile) dataset = list(lines) for x in range(len(dataset)-1): for y in range(4): dataset[x][y] = float(dataset[x][y]) if random.random() < split: trainingSet.append(dataset[x]) else: testSet.append(dataset[x]) def euclideanDistance(instance1, instance2, length): distance = 0 for x in range(length): distance += pow((instance1[x] - instance2[x]), 2) return math.sqrt(distance) def getNeighbors(trainingSet, testInstance, k): distances = [] length = len(testInstance)-1 for x in range(len(trainingSet)): dist = euclideanDistance(testInstance, trainingSet[x], length) distances.append((trainingSet[x], dist)) distances.sort(key=operator.itemgetter(1)) neighbors = [] for x in range(k): neighbors.append(distances[x][0]) return neighbors def getResponse(neighbors): classVotes = {} for x in range(len(neighbors)): response = neighbors[x][-1] if response in classVotes: classVotes[response] += 1 else: classVotes[response] = 1 sortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedVotes[0][0] def getAccuracy(testSet, predictions): correct = 0 for x in range(len(testSet)): if testSet[x][-1] == predictions[x]: correct += 1 return (correct/float(len(testSet))) * 100.0 def main(): # prepare data trainingSet=[] testSet=[] split = 0.67 loadDataset('iris.data', split, trainingSet, testSet) print 'Train set: ' + repr(len(trainingSet)) print 'Test set: ' + repr(len(testSet)) # generate predictions predictions=[] k = 3 for x in range(len(testSet)): neighbors = getNeighbors(trainingSet, testSet[x], k) result = getResponse(neighbors) predictions.append(result) print('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1])) accuracy = getAccuracy(testSet, predictions) print('Accuracy: ' + repr(accuracy) + '%') main() Menjalankan contoh, Anda akan melihat hasil dari setiap prediksi dibandingkan dengan nilai kelas aktual di set tes. Di akhir proses, Anda akan melihat keakuratan model. Dalam hal ini, sedikit di atas 98%. ... > predicted='Iris-virginica', actual='Iris-virginica' > predicted='Iris-virginica', actual='Iris-virginica' > predicted='Iris-virginica', actual='Iris-virginica' > predicted='Iris-virginica', actual='Iris-virginica' > predicted='Iris-virginica', actual='Iris-virginica' Accuracy: 98.0392156862745%","title":"Material"},{"location":"#k-nearest-neighbor","text":"","title":"K-NEAREST NEIGHBOR"},{"location":"#pengertian-k-nearest-neighbor","text":"Algoritme k-nearest neighbor (k-NN atau KNN) adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data pembelajaran. Sebuah titik pada ruang ini ditandai kelas c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean. Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah k buah yang paling dekat diambil. Titik yang baru klasifikasinya diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik tersebut. Nilai k yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritme nearest neighbor . Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu).","title":"Pengertian K-NEAREST NEIGHBOR"},{"location":"#algoritma-k-nearest-neighbor","text":"terdapat beberapa data yang menunjukkan wilayah kependudukan berdasarkan latitude dan longitude (atau garis lintang dan garis bujur) sebagai posisi untuk menentukan wilayah tersebut termasuk kota atau kabupaten pada wilayah bandung Dari data diatas, kita mendapatkan beberapa informasi, diantaranya: Independent Variables , yaitu variable yang nilainya tidak dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk independent variable adalah Lat , dan Long . Dependent Variables , yaitu variable yang nilainya dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk dependent variable adalah Lokasi . Rumah A-E adalah rumah yang masuk ke dalam wilayah Kota . Rumah F-G adalah rumah yang masuk ke dalam wilayah Kabupaten . Rumah X adalah rumah yang akan kita prediksi menggunakan algoritma kNN apakah termasuk ke dalam wilayah Kota atau Kabupaten. Didalam dunia Machine Learning , Independent Variables sering disebut juga sebagai Features . Selanjutnya kita hitung jarak antara rumah X terhadap rumah A-G dengan menggunakan rumus pythagoras : Diketahui, dimana x adalah Lat , y adalah Long , sedangkan (x1, y1) adalah lat dan long dari rumah X , dan (x2, y2) adalah lat dan long dari masing-masing tetangganya . Setelah dihitung, selanjutnya adalah urutkan jarak tersebut dari yang paling kecil ke yang paling besar , hasilnya adalah sebagai berikut: Dapat dilihat dari hasil perhitungan diatas, bahwa ternyata 3 tetangga terdekat dari rumah X adalah: Rumah H (Kabupaten) yang memiliki jarak 2.24 , Rumah C (Kota) yang memiliki jarak 3 , dan Rumah E (Kota) yang memiliki jarak 3.16 . Dari ke-3 tetangga terdekat, terdapat 2 rumah yang termasuk kedalam wilayah Kota dan 1 rumah yang masuk ke dalam wilayah Kabupaten . Sehingga dapat disimpulkan, bahwa Rumah X adalah rumah yang termasuk kedalam wilayah Kota Bandung .","title":"Algoritma K-NEAREST NEIGHBOR"},{"location":"#implementasi-k-nearest-neighbor-pada-python","text":"hal yang diperlukan dalam pengimplementasian python 3.6 download data pada laman berikut https://github.com/BULLZHIT/data/tree/master/knn","title":"Implementasi K-NEAREST NEIGHBOR pada python"},{"location":"#algoritma-k-mean-clustering-pada-python","text":"setelah syarat terpunuhi kita dapat memulai pada langkah selanjutnya Import Data Hal pertama yang perlu kita lakukan adalah memuat file data kita. Data dalam format CSV tanpa baris header atau kutipan. Kita dapat membuka file dengan fungsi terbuka dan membaca baris data menggunakan fungsi pembaca dalam modul csv. import csv with open('iris.data', 'rb') as csvfile: lines = csv.reader(csvfile) for row in lines: print ', '.join(row) Selanjutnya kita perlu membagi data menjadi dataset training yang dapat digunakan kNN untuk membuat prediksi dan dataset data testing yang dapat kita gunakan untuk mengevaluasi akurasi model. Pertama-tama kita perlu mengubah ukuran bunga yang dimuat sebagai string menjadi angka yang dapat kita kerjakan. Selanjutnya kita perlu membagi set data secara acak menjadi kereta dan dataset. Rasio 67/33 untuk ujicoba/ tes adalah rasio standar yang digunakan. Dengan menggabungkan semuanya, kita dapat membuat fungsi yang disebut loadDataset yang memuat CSV dengan nama file yang disediakan dan membaginya secara acak ke dalam train dan menguji dataset menggunakan rasio split yang disediakan. import csv import random def loadDataset(filename, split, trainingSet=[] , testSet=[]): with open(filename, 'rb') as csvfile: lines = csv.reader(csvfile) dataset = list(lines) for x in range(len(dataset)-1): for y in range(4): dataset[x][y] = float(dataset[x][y]) if random.random() < split: trainingSet.append(dataset[x]) else: testSet.append(dataset[x]) Unduh file CSV dataset dataset bunga iris ke direktori lokal. Kami dapat menguji fungsi ini dengan set data iris kami, sebagai berikut: trainingSet=[] testSet=[] loadDataset('iris.data', 0.66, trainingSet, testSet) print 'Train: ' + repr(len(trainingSet)) print 'Test: ' + repr(len(testSet)) Kesamaan Untuk membuat prediksi kita perlu menghitung kesamaan antara dua instance data yang diberikan. Ini diperlukan agar kita dapat menemukan contoh instans k yang paling mirip dalam dataset pelatihan untuk anggota tertentu dari dataset pengujian dan pada gilirannya membuat prediksi. Mengingat keempat pengukuran bunga bersifat numerik dan memiliki satuan yang sama, kita dapat langsung menggunakan ukuran jarak Euclidean. Ini didefinisikan sebagai akar kuadrat dari jumlah perbedaan kuadrat antara dua array angka (baca lagi beberapa kali dan biarkan meresap). Selain itu, kami ingin mengontrol bidang mana yang akan dimasukkan dalam perhitungan jarak. Secara khusus, kami hanya ingin menyertakan 4 atribut pertama. Salah satu pendekatan adalah membatasi jarak euclidean ke panjang tetap, mengabaikan dimensi akhir. Menyatukan semua ini, kita dapat mendefinisikan fungsi euclideanDistance sebagai berikut: import math def euclideanDistance(instance1, instance2, length): distance = 0 for x in range(length): distance += pow((instance1[x] - instance2[x]), 2) return math.sqrt(distance) kita dapat menguji fungsi ini dengan beberapa data sampel, sebagai berikut: data1 = [2, 2, 2, 'a'] data2 = [4, 4, 4, 'b'] distance = euclideanDistance(data1, data2, 3) print 'Distance: ' + repr(distance) Tetangga Sekarang kita memiliki ukuran kesamaan, kita dapat menggunakannya mengumpulkan k contoh yang paling mirip untuk contoh yang tidak terlihat. Ini adalah proses langsung menghitung jarak untuk semua instance dan memilih subset dengan nilai jarak terkecil. Di bawah ini adalah fungsi getNeighbors yang mengembalikan k paling mirip tetangga dari set pelatihan untuk contoh uji yang diberikan (menggunakan fungsi euclideanDistance yang sudah ditentukan) import operator def getNeighbors(trainingSet, testInstance, k): distances = [] length = len(testInstance)-1 for x in range(len(trainingSet)): dist = euclideanDistance(testInstance, trainingSet[x], length) distances.append((trainingSet[x], dist)) distances.sort(key=operator.itemgetter(1)) neighbors = [] for x in range(k): neighbors.append(distances[x][0]) return neighbors Kita dapat menguji fungsi ini sebagai berikut: trainSet = [[2, 2, 2, 'a'], [4, 4, 4, 'b']] testInstance = [5, 5, 5] k = 1 neighbors = getNeighbors(trainSet, testInstance, 1) print(neighbors) Respon Setelah kami menemukan tetangga yang paling mirip untuk contoh pengujian, tugas selanjutnya adalah menyusun respons yang diprediksi berdasarkan tetangga tersebut. Kita dapat melakukan ini dengan mengizinkan setiap tetangga untuk memilih atribut kelas mereka, dan mengambil suara mayoritas sebagai prediksi. Di bawah ini menyediakan fungsi untuk mendapatkan respons suara terbanyak dari sejumlah tetangga. Diasumsikan kelas adalah atribut terakhir untuk setiap tetangga. import operator def getResponse(neighbors): classVotes = {} for x in range(len(neighbors)): response = neighbors[x][-1] if response in classVotes: classVotes[response] += 1 else: classVotes[response] = 1 sortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedVotes[0][0] Kami dapat menguji fungsi ini dengan beberapa tetangga pengujian, sebagai berikut: neighbors = [[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']] response = getResponse(neighbors) print(response) Akurasi Kami memiliki semua bagian dari algoritma kNN di tempatnya. Kekhawatiran penting yang tersisa adalah bagaimana mengevaluasi keakuratan prediksi. Cara mudah untuk mengevaluasi keakuratan model adalah dengan menghitung rasio total prediksi yang benar dari semua prediksi yang dibuat, yang disebut akurasi klasifikasi. Di bawah ini adalah fungsi getAccuracy yang menjumlahkan total prediksi yang benar dan mengembalikan akurasi sebagai persentase dari klasifikasi yang benar. def getAccuracy(testSet, predictions): correct = 0 for x in range(len(testSet)): if testSet[x][-1] is predictions[x]: correct += 1 return (correct/float(len(testSet))) * 100.0 kita dapat menguji fungsi ini dengan dataset uji dan prediksi, sebagai berikut: testSet = [[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']] predictions = ['a', 'a', 'a'] accuracy = getAccuracy(testSet, predictions) print(accuracy) Main Kita sekarang memiliki semua elemen algoritma dan Kita dapat mengikatnya bersama dengan fungsi utama. Di bawah ini adalah contoh lengkap penerapan algoritma kNN dari awal dengan Python. # Example of kNN implemented from Scratch in Python import csv import random import math import operator def loadDataset(filename, split, trainingSet=[] , testSet=[]): with open(filename, 'rb') as csvfile: lines = csv.reader(csvfile) dataset = list(lines) for x in range(len(dataset)-1): for y in range(4): dataset[x][y] = float(dataset[x][y]) if random.random() < split: trainingSet.append(dataset[x]) else: testSet.append(dataset[x]) def euclideanDistance(instance1, instance2, length): distance = 0 for x in range(length): distance += pow((instance1[x] - instance2[x]), 2) return math.sqrt(distance) def getNeighbors(trainingSet, testInstance, k): distances = [] length = len(testInstance)-1 for x in range(len(trainingSet)): dist = euclideanDistance(testInstance, trainingSet[x], length) distances.append((trainingSet[x], dist)) distances.sort(key=operator.itemgetter(1)) neighbors = [] for x in range(k): neighbors.append(distances[x][0]) return neighbors def getResponse(neighbors): classVotes = {} for x in range(len(neighbors)): response = neighbors[x][-1] if response in classVotes: classVotes[response] += 1 else: classVotes[response] = 1 sortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedVotes[0][0] def getAccuracy(testSet, predictions): correct = 0 for x in range(len(testSet)): if testSet[x][-1] == predictions[x]: correct += 1 return (correct/float(len(testSet))) * 100.0 def main(): # prepare data trainingSet=[] testSet=[] split = 0.67 loadDataset('iris.data', split, trainingSet, testSet) print 'Train set: ' + repr(len(trainingSet)) print 'Test set: ' + repr(len(testSet)) # generate predictions predictions=[] k = 3 for x in range(len(testSet)): neighbors = getNeighbors(trainingSet, testSet[x], k) result = getResponse(neighbors) predictions.append(result) print('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1])) accuracy = getAccuracy(testSet, predictions) print('Accuracy: ' + repr(accuracy) + '%') main() Menjalankan contoh, Anda akan melihat hasil dari setiap prediksi dibandingkan dengan nilai kelas aktual di set tes. Di akhir proses, Anda akan melihat keakuratan model. Dalam hal ini, sedikit di atas 98%. ... > predicted='Iris-virginica', actual='Iris-virginica' > predicted='Iris-virginica', actual='Iris-virginica' > predicted='Iris-virginica', actual='Iris-virginica' > predicted='Iris-virginica', actual='Iris-virginica' > predicted='Iris-virginica', actual='Iris-virginica' Accuracy: 98.0392156862745%","title":"Algoritma K-Mean Clustering pada python"}]}